```markdown
# Assignment 2 â€“ Language Model Training (Underfit, Overfit & Best Fit)

This project implements and compares three training behaviors of a simple **Neural Language Model** using **PyTorch**:

- **Underfitting**
- **Overfitting**
- **Best Fit (Optimal Generalization)**

The objective is to understand how training configuration, dataset size, and model capacity affect generalization.

---

## ğŸ“ Project Structure

```

Assignment2/
â”‚â”€â”€ data/
â”‚   â””â”€â”€ input.txt
â”‚
â”‚â”€â”€ model.py
â”‚â”€â”€ utils.py
â”‚â”€â”€ train.py
â”‚
â”‚â”€â”€ plots/
â”‚   â”œâ”€â”€ loss_underfit.png
â”‚   â”œâ”€â”€ loss_overfit.png
â”‚   â”œâ”€â”€ loss_bestfit.png
â”‚
â””â”€â”€ README.md

````

---

## ğŸš€ How to Run the Project

### 1ï¸âƒ£ Install Dependencies
```bash
pip install torch matplotlib numpy
````

### 2ï¸âƒ£ Choose the Training Scenario

Open **train.py** and set:

```python
EXPERIMENT = "underfit"
# or
EXPERIMENT = "overfit"
# or
EXPERIMENT = "bestfit"
```

### 3ï¸âƒ£ Run the Training

```bash
python train.py
```

Each run will automatically save the corresponding plot in the **plots/** folder.

---

# ğŸ“Š Training Result Plots

## âœ… Best Fit

Training loss and validation loss decrease steadily â†’ good generalization.

![Best Fit](plots/loss_bestfit.png)

---

## âŒ Overfit

Training loss keeps decreasing, but validation loss increases â†’ model memorizes data.

![Overfit](plots/loss_overfit.png)

---

## âš  Underfit

Both training and validation losses remain high â†’ model too simple or trained too little.

![Underfit](plots/loss_underfit.png)

---

# ğŸ§  Summary of the Three Scenarios

| Scenario     | Train Loss      | Validation Loss            | Explanation                                           |
| ------------ | --------------- | -------------------------- | ----------------------------------------------------- |
| **Underfit** | Slight decrease | Stagnant / slightly rising | Model is too simple or training is too short          |
| **Overfit**  | Very low        | High and rising            | Model memorizes training data but fails to generalize |
| **Best Fit** | Smooth decrease | Smooth decrease            | Balanced capacity â†’ best performance                  |

---

# ğŸ›  Code Overview

## `model.py`

Defines the neural network architecture:

* Embedding layer
* Hidden linear layer
* ReLU activation
* Output projection

## `utils.py`

Handles:

* Text preprocessing
* Dataset batching
* Train/validation split

## `train.py`

Responsible for:

* Loading data
* Selecting experiment type
* Running training loop
* Saving loss plots

---

# ğŸ¯ Learning Outcomes

By completing this assignment, you learn:

* What causes **underfitting** and **overfitting**
* How to control model capacity and epochs
* How to evaluate model performance using **loss curves**
* How training settings impact generalization

---

# âœ¨ Author

Nikshitha Kompally

---



```
